{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    ''' zip 파일에 포함된 텍스트 파일을 읽어서 단어 리스트 생성. 포함된 파일은 1개. \n",
    "    zip 파일은 30mb, txt 파일은 100mb. '''\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        names = f.namelist()                # ['text8']\n",
    "        contents = f.read(names[0])         # 크기 : 100,000,000바이트\n",
    "        text = tf.compat.as_str(contents)   # 크기 : 100,000,000\n",
    "        return text.split()                 # 갯수 : 17005207\n",
    "\n",
    "\n",
    "vocabulary = read_data(filename)\n",
    "print('Data size', len(vocabulary))   \n",
    "\n",
    "# .을 기준으로 리스트 여러개로 나눈후에 space로 다시 리스트로 나눈다.\n",
    "# I like you. you like me -> [[I like you], [you like me]] -> [[[I] [like] [you]], ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_numbering(vocabulary, number_of_n_words=50000):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    vocabulary -- a list of words you want to train the word2vec\n",
    "    number_of_n_words -- number of most frequent n words in the vocabulary you want to set\n",
    "    \n",
    "    Returns:\n",
    "    int_voc -- a list of vocabulary that is mapped into integer-valeud index\n",
    "    word_to_int -- python dict mapping words in the vocabulary into an integer-valued index\n",
    "    int_to_word -- python dict mapping integer-valued index to words\n",
    "    most_frequent_n_words -- a list with pairs of n most frequent words with its frequency -> [(word, frequency)]\n",
    "    \"\"\"\n",
    "    word_count = collections.Counter(vocabulary)\n",
    "    most_frequent_n_words = word_count.most_common(number_of_n_words - 1)\n",
    "            \n",
    "    word_to_int = {'UNK': 0} # set 0 as unknown token\n",
    "    for word, _ in most_frequent_n_words:\n",
    "        word_to_int[word] = len(word_to_int)\n",
    "    \n",
    "    int_to_word = {v: k for k, v in word_to_int.items()} # reverse dict of word_to_int\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    int_voc = []\n",
    "\n",
    "    for word in vocabulary:\n",
    "        if word in word_to_int: # if word is in n most frequent words\n",
    "            int_voc.append(word_to_int[word]) #. change the word into the corresponding integer\n",
    "        else:\n",
    "            int_voc.append(0) # not in the n most frequent word.\n",
    "            count += 1   # number of words that are not in n most frequent words\n",
    " \n",
    "    most_frequent_n_words.insert(0, ('UNK', count))\n",
    "    #most_frequent_n_words = np.insert(most_frequent_n_words, 0,('UNK', count),axis=0)\n",
    "            \n",
    "    return int_voc, word_to_int, int_to_word, most_frequent_n_words\n",
    "    \n",
    "# del vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the test\n",
    "int_voc, word_to_int, int_to_word, most_frequent_n_words = word_numbering(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f(most_frequent_n_words):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    total = sum([most_frequent_n_words[x][1] for x in range(len(most_frequent_n_words))]) # compute the vocabulary len.\n",
    "    \n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsampling(int_voc, most_frequent_n_words, word, t=0.00001):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    most_frequent_n_words --\n",
    "    t --\n",
    "    \n",
    "    Returns:\n",
    "    p_w_i\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the test\n",
    "total = sum([most_frequent_n_words[x][1] for x in range(len(most_frequent_n_words))])\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [(1,2),(2,1),(3,1)]\n",
    "b = np.array(a)\n",
    "b[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061396.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "most_frequent_n_words[1][1].astype(np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(int_voc, batch_size, window_size, num_context_word, start_x, start_y):\n",
    "    \"\"\"\n",
    "    Create mini batch size of batch_size with each batch containing num_target_word training examples.\n",
    "    \n",
    "    Arguments:\n",
    "    data -- a list of vocabulary that is mapped into integer-valeud index\n",
    "    batch_size -- size of batch in stochastic gradient descent\n",
    "    window_size -- size of window around the target word.\n",
    "    num_context_word --\n",
    "    \n",
    "    Returns:\n",
    "    start_x\n",
    "    start_y\n",
    "    \"\"\"\n",
    "    int_voc = [int_voc] # this will deleted if the input includes punctuation\n",
    "    \n",
    "    assert (start_y < len(int_voc[start_x]))\n",
    "\n",
    "    for i in range(batch_size / num_context_word):\n",
    "        start = max(0, start_y - window_size) \n",
    "        end = min(start_y + window_size, len(int_voc[start_x]) - 1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: skip-gram 모델에 사용할 학습 데이터를 생성할 함수 작성\n",
    "\n",
    "# 원본에서는 data_index를 전역변수로 처리했는데, 여기서는 매개변수와 반환값으로 변경했다.\n",
    "# data 변수 또한 전역변수였는데, 첫 번째 매개변수로 전달하도록 변경했다.\n",
    "def generate_batch(data, batch_size, num_skips, skip_window, data_index):\n",
    "    ''' Stochastic Gradient Descent 알고리즘에 사용할 minibatch 생성.\n",
    "    :param data : 단어 인덱스 리스트\n",
    "    :param batch_size : SGD 알고리즘에 적용할 데이터 갯수. 한 번에 처리할 크기.\n",
    "    :param num_skips : context window에서 구축할 (target, context) 쌍의 갯수.\n",
    "    :param skip_window : skip-gram 모델에 사용할 윈도우 크기.\n",
    "         1이라면 목표 단어(target) 양쪽에 1개 단어이므로 context window 크기는 3이 된다. (단어, target, 단어)\n",
    "         2라면 5가 된다. (단어 단어 target 단어 단어)\n",
    "    :param data_index : 첫 번째 context window에 들어갈 data에서의 시작 위치.\n",
    "    '''\n",
    "    # 조건이 False라면 비정상 종료\n",
    "    # num_skips <= batch_size. batch_size는 num_skips의 정수 배.\n",
    "    # num_skips는 skip_window의 2배 이하.\n",
    "    # num_skips가 skip_window의 2배일 때, context 윈도우의 모든 쌍 구성\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "\n",
    "    temp = 'batch_size {}, num_skips {}, skip_window {}, data_index {}'\n",
    "    # 최초 : batch_size 8, num_skips 2, skip_window 1, data_index 0\n",
    "    # 학습 : batch_size 128, num_skips 2, skip_window 1, data_index 640000\n",
    "    #       data_index는 64로 시작해서 64씩 증가한다. 나머지는 변경되지 않는다.\n",
    "    # print(temp.format(batch_size, num_skips, skip_window, data_index))\n",
    "\n",
    "    # ndarray에 값을 주지 않았다면 난수가 사용된다. 앞부분 10개만 표시.\n",
    "    # batch는 1차원, labels는 2차원.\n",
    "    # batch  : [0 0 268907754 536870912 -1354956798 32767 32229680 131073]\n",
    "    # labels : [[0] [0] [268799296] [-805306368] [2] [0] [268811838] [131072]]\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "    # span은 assert에 기술한 코드 때문에 항상 num_skips보다 크다.\n",
    "    span = 2 * skip_window + 1                      # context = skip_window + target + skip_window\n",
    "    assert span > num_skips\n",
    "\n",
    "    # deque\n",
    "    # 처음과 마지막의 양쪽 끝에서 일어나는 입출력에 대해 가장 좋은 성능을 내는 자료구조\n",
    "    # maxlen 옵션이 없으면 크기 제한도 없고, 있다면 지정한 크기만큼만 사용 가능.\n",
    "    # maxlen을 3으로 전달하면 3개만 저장할 수 있고, 새로운 요소를 추가하면 반대쪽 요소가 자동으로 삭제됨.\n",
    "    # 여기서는 자동 삭제 기능 때문에 사용.\n",
    "    # data_index 번째부터 span 크기만큼 단어 인덱스 저장\n",
    "    # 첫 번째 context 윈도우 구성\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)   # 다음 단어 인덱스로 이동. len(data) = 17005207\n",
    "\n",
    "    # 위의 코드를 재구성(에러).\n",
    "    # data_index는 마지막에서 처음으로 cycle을 구성하기 때문에 오류 발생할 수 있다.\n",
    "    # buffer = collections.deque(data[data_index:data_index+span], maxlen=span)\n",
    "    # data_index = (data_index + span) % len(data)\n",
    "\n",
    "    # skip-gram은 타겟 단어로부터 주변의 컨텍스트 단어를 예측하는 모델이다.\n",
    "    # 학습하기 전에 단어들을 (target, context) 형태로 변환해 주어야 한다.\n",
    "    # 바깥 루프는 batch_size // num_skips\n",
    "    # 안쪽 루프는 num_skips\n",
    "    # batch_size는 num_skips로 나누어 떨어지기 때문에 정확하게 batch_size만큼 반복\n",
    "    for i in range(batch_size // num_skips):\n",
    "\n",
    "        # 원본 코드\n",
    "        # target = skip_window               # target label은 buffer의 가운데 위치\n",
    "        # targets_to_avoid = [skip_window]\n",
    "        #\n",
    "        # for j in range(num_skips):\n",
    "        #     while target in targets_to_avoid:\n",
    "        #         target = random.randrange(span)\n",
    "        #     targets_to_avoid.append(target)\n",
    "        #     batch[i * num_skips + j] = buffer[skip_window]\n",
    "        #     labels[i * num_skips + j, 0] = buffer[target]\n",
    "        #     print(target, '**')       # (2, 0), (0, 2), (0, 2), (0, 2)\n",
    "\n",
    "        # 재구성한 코드\n",
    "        # skip_window는 context의 가운데 위치한 단어의 인덱스\n",
    "        # skip_window가 3이라면 주변에 3개의 단어씩 위치하게 되고 3+1+3은 7개로 구성된 context가 만들어진다.\n",
    "        # 읽어올 데이터가 0부터 시작한다면 skip_window는 3이 되고, context의 가운데에 위치한다.\n",
    "        # 값을 생성할 때, num_skips와 span 중에서 신중하게 선택해야 한다.\n",
    "        # num_skips는 context로부터 구성할 단어들의 갯수이기 때문에\n",
    "        # num_skips를 사용하면 context에 포함된 단어들의 모든 인덱스가 반영되지 않을 수 있다.\n",
    "        # skip_window*2 == num_skips 일 때는 모든 단어를 사용하기 때문에 난수를 사용할 필요가 없다.\n",
    "        # 여기서는 항상 skip_window는 1, num_skips는 2의 값을 갖는다.\n",
    "        targets = list(range(span))     # 1. 0부터 span-1까지의 정수로 채운 다음\n",
    "        targets.pop(skip_window)        # 2. skip_window번째 삭제\n",
    "        np.random.shuffle(targets)      # 3. 난수를 사용해서 섞는다.\n",
    "\n",
    "        # batch : target 단어만 들어가고, num_skips만큼 같은 단어가 중복된다.\n",
    "        # labels : target을 제외한 단어만 들어가고, num_skips만큼 중복될 수 있다.\n",
    "        start = i * num_skips\n",
    "        batch[start:start+num_skips] = buffer[skip_window]\n",
    "\n",
    "        # span이 큰 값이기 때문에 targets에 포함된 모든 값을 사용하지 않을 수 있다.\n",
    "        # buffer는 numpy 데이터가 아니라서 슬라이스 연산 불가. 어쩔 수 없이 반복문 사용.\n",
    "        # numpy 배열을 사용하면서 마지막에 있는 num_skips 갯수만큼 사용하는 것이 나을 수도 있다.\n",
    "        for j in range(num_skips):\n",
    "            labels[start+j, 0] = buffer[targets[j]]\n",
    "            # print(targets[j], '**')     # (2, 0), (0, 2), (0, 2), (0, 2)\n",
    "\n",
    "        # 새로운 요소가 들어가면서 가장 먼저 들어간 데이터 삭제\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    # 아래는 generate_batch 함수를 최초 호출한 결과.\n",
    "    # 오른쪽 출력 결과를 보면, batch와 labels의 관계를 알 수 있다.\n",
    "    # print(buffer)           # deque([195, 2, 3134], maxlen=3)\n",
    "    # print(data[:20])        # [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n",
    "    # print(batch[:20])       # [3081 3081   12   12    6    6  195  195]\n",
    "    # print(labels[:20])      # [[5234] [12] [6] [3081] [195] [12] [2] [6]]\n",
    "\n",
    "    # data_index는 반복문에서 batch_size // num_skips 만큼 증가한다.\n",
    "    # 최정적인 data_index는 마지막을 지난 위치를 가리키게 되니까, 정확한 계산을 위해 앞으로 돌려놓을 필요가 있다.\n",
    "    # span에 context 윈도우 전체 크기가 있으니까, span만큼 뒤로 이동한다.\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels, data_index\n",
    "\n",
    "\n",
    "# generate_batch 함수 테스트. 아래 코드는 뒤쪽에서 사용하지 않는다.\n",
    "batch, labels, data_index = generate_batch(data, batch_size=8, num_skips=2, skip_window=1, data_index=0)\n",
    "for i in range(8):\n",
    "    print('{} {} -> {} {}'.format(batch[i],     ordered_words[batch[i]],\n",
    "                                  labels[i, 0], ordered_words[labels[i, 0]]))\n",
    "# [출력 결과]\n",
    "# 3081 originated -> 12 as\n",
    "# 3081 originated -> 5234 anarchism\n",
    "# 12 as -> 6 a\n",
    "# 12 as -> 3081 originated\n",
    "# 6 a -> 195 term\n",
    "# 6 a -> 12 as\n",
    "# 195 term -> 2 of\n",
    "# 195 term -> 6 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_initialization(vocabulary_size, projection_size):\n",
    "    \"\"\"\n",
    "    Initialization method can be changed. \n",
    "    \n",
    "    Argument:\n",
    "    vocabulary_size -- the size of the vocabulary\n",
    "    projection_size -- the dimension of hidden layer in NN\n",
    "    \n",
    "    Return:\n",
    "    parameters -- a python dictionary containing weights W_in, b_in, W_out, b_out\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    parameters = {}\n",
    "    \n",
    "    parameters['W_in'] = np.random.randn(vocabulary_size, projection_size) * 0.01 \n",
    "    # parameters[\"b_in\"] = np.zeros((projection_size, 1))\n",
    "    \n",
    "    parameters['W_out'] = np.random.randn(projection_size, vocabulary_size) * 0.01\n",
    "    # parameters['b_out'] = np.zeros((vocabulary_size, 1))\n",
    "    \n",
    "    assert (parameters['W_in'].shape == (vocabulary_size, projection_size))\n",
    "    assert (parameters['W_out'].shape == (projection_size, vocabulary_size))\n",
    "    \n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_NN(batch, labels, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    batch_input -- input of Neural Network\n",
    "    layers -- a list containing the dimensions of each layer\n",
    "    parameters -- python dictionary containing weights W_in, b_in, W_out, b_out\n",
    "    \n",
    "    Returns:\n",
    "    cost -- the cost function of the NN\n",
    "    caches -- used in backpropagation\n",
    "    \"\"\"\n",
    "    W_in = parameters['W_in']  # size of vocabulary_size, projection_size\n",
    "    hidden = W_in.T[:, batch]\n",
    "    W_out = parameters['W_out']\n",
    "    output = np.dot(W_out.T, hidden)\n",
    "    # need to implement negative sampling\n",
    "    # b_in = parameters['b_in']\n",
    "    # b_out = parameters['b_out']\n",
    "    \n",
    "    assert(hidden.shape == (W_in.shape[1], len(batch)))\n",
    "    assert(output.shape == (W_out.shape[1], len(batch)))\n",
    "    \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function(output, labels):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    output -- the prediction vector shape of (vocabulary_size, batch_size)\n",
    "    label -- true label vector, shape of (batch_size, 1)\n",
    "    \n",
    "    Returns: \n",
    "    cost -- cost function shape of ()\n",
    "    \"\"\"\n",
    "    cost = np.sum(output + labels[1]) # needs to be changed.\n",
    "    \n",
    "    np.squeeze(cost)\n",
    "    assert (cost.shape == ())\n",
    "    \n",
    "    \n",
    "    return cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation():\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_Rate):\n",
    "    \"\"\"\n",
    "    Updates the parameters based on adaGradientDescent with learning rate decay\n",
    "    Arguments:\n",
    "    parameters -- python dict containing parameters before the updates\n",
    "    grads -- python dict containing the gradient\n",
    "    Returns:\n",
    "    parameters -- python dict containing updated parameters \n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(vocabulary, projection_size, num_of_n_words, epoch, learning_rate):\n",
    "    \n",
    "    \n",
    "    parameters = parameter_initialization(vocabulary_size, projection_size)\n",
    "    \n",
    "    int_voc, word_to_int, int_to_word, most_frequent_n_words = word_numbering(vocabulary)\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        # train\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    # count : [['UNK', -1], ('the', 1061396), ('of', 593677), ('and', 416629), ...]\n",
    "    # 크기는 50,000개. UNK가 들어 있고, -1을 뺐으니까 처음에 전달된 크기 사용.\n",
    "    # 빈도가 높은 5만개 추출.\n",
    "    # count에 포함된 마지막 데이터는 ('hif', 9). 9번 나왔는데 드물다고 얘기할 수 있는지는 의문.\n",
    "    unique = collections.Counter(words)             # 중복 단어 제거\n",
    "    orders = unique.most_common(n_words - 1)        # 단어에 대한 빈도 계산. 갯수를 지정하지 않으면 전체 계산.\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(orders)\n",
    "\n",
    "    # dictionary : (UNK, 0) (the, 1) (of, 2) (and, 3) (one, 4) (in, 5) (a, 6) (to, 7)\n",
    "    # 내용을 보면 단어에 번호를 매겼다는 것을 알 수 있다.\n",
    "    dictionary = {}\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    # 위의 코드는 결국 0부터 1씩 증가하는 인덱스를 len(dictionary)로 표현했기 때문에\n",
    "    # enumerate 함수를 사용한 아래처럼 표현할 수 있다. len(dictionary)는 코드가 모호하다.\n",
    "    # for i, (word, _) in enumerate(count):\n",
    "    #     dictionary[word] = i\n",
    "\n",
    "    # dictionary = {word: i for i, (word, _) in enumerate(count)}\n",
    "\n",
    "    # 단어 전체에 대해 인덱스 매핑. data는 단어를 가리키는 인덱스 리스트가 된다.\n",
    "    # 인덱스를 계산하기 위해 딕셔너리 대신 리스트를 사용할 수도 있고, 얼핏 보면 좋아보일 수도 있다.\n",
    "    # 리스트를 사용하면 이진 검색을 적용해야\n",
    "    data = []\n",
    "    for word in words:\n",
    "        if word in dictionary:          # word가 dictionary에 존재한다면\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0                   # UNK는 0번째에 위치\n",
    "            count[0][1] += 1            # 갯수 : 418391\n",
    "        data.append(index)\n",
    "\n",
    "    # dictionary와 reversed_dictionary 내용\n",
    "    # 일련번호로 된 key와 value가 의미 있을까? 리스트로 처리하면 되지 않을까?\n",
    "    # (UNK, 0) (the, 1) (of, 2) (and, 3) (one, 4) (in, 5) (a, 6) (to, 7) (zero, 8) (nine, 9) (two, 10)\n",
    "    # (0, UNK) (1, the) (2, of) (3, and) (4, one) (5, in) (6, a) (7, to) (8, zero) (9, nine) (10, two)\n",
    "    # reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "    # reversed_dictionary는 0부터 시작하는 일련번호를 갖기 때문에 딕셔너리로 만들면 오히려 어렵고 불편하다.\n",
    "    # 아래 코드는 리스트와 같다는 것을 증명하는 코드.\n",
    "    # a = list(dictionary.values())\n",
    "    # print(type(a), len(a))\n",
    "    # print(a[0], a[-1])\n",
    "    #\n",
    "    # b = list(range(len(dictionary.values())))\n",
    "    # print(b[0], b[-1])\n",
    "    #\n",
    "    # assert a == b\n",
    "\n",
    "    # reversed_dictionary 대신 key로 구성된 리스트 반환.\n",
    "    # [(0, UNK) (1, the) (2, of) (3, and) (4, one)]에서 인덱스를 제외하고 구성한 리스트.\n",
    "    # 원본에서는 dictionary 변수를 반환하고 있는데, 사용하지 않기 때문에 삭제.\n",
    "    return data, count, list(dictionary.keys())\n",
    "\n",
    "# data : 단어에 대한 인덱스만으로 구성된 리스트\n",
    "# count : 단어와 빈도 쌍으로 구성된 리스트. 중요한 변수이지만, 이번 코드에서는 사용 안함.\n",
    "# ordered_words : 빈도에 따라 정렬된 단어 리스트\n",
    "data, count, ordered_words = build_dataset(vocabulary, vocabulary_size)\n",
    "\n",
    "# print('Most common words (+UNK)', count[:5])\n",
    "# print('Sample data', data[:10], [ordered_words[i] for i in data[:10]], sep='\\n')\n",
    "# print('-'*50)\n",
    "del vocabulary, count               # 사용하지 않는 변수 삭제\n",
    "\n",
    "# [출력 결과]\n",
    "# Most common words (+UNK) [['UNK', 418390], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
    "# Sample data\n",
    "# [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n",
    "# ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
